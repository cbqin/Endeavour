
# 特征工程

## 数值型特征：特征归一化

数值特征存在量纲，归一化可以使特征都统一到一个大致相同的数值区间。

- 线性函数归一化 (Min-Max Scaling)

   将原始数据映射到 [0,1] ，等比缩放。公式如下：
   $$X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}$$

- 零均值归一化 (Z-Score Normalization)

   将原始数据映射到均值为 0 ，方差为 1 的分布上。公式如下：
   $$z=\frac{x-\mu}{\delta}$$

以梯度下降为例，归一化可以使特征的更新速度变得一致，更快地找到最优解。
一般需要使用梯度下降的模型是需要归一化的，而决策树则不适用。

## 类别型特征

一般需要转换成数值型特征。决策树不需要。

- 序号编码

  类别具有大小关系。例如：成绩 高、中、低 编码为 3、2、1。

- 独热编码

  类别不具有大小关系。例如词表。需要注意的问题：
  
  - 使用稀疏向量表示，节省空间
  - 配合特征选择降低维度，因为高维度特征会有些问题：

    - K 近邻算法中，高维度空间中两点之间的距离无法得到有效地衡量
    - 在逻辑回归中，参数的数量会随着维度的增高而增加，容易引起过拟合
    - 不是所有维度都会对预测、分类起作用

- 二进制编码

  先用序号编码赋予每一类别一个 ID ，然后将 ID 转换为二进制形式。维度少于独热编码，节省存储空间。

## 组合特征

为了提高复杂关系的拟合能力，在特征工程中经常把一阶离散特征两两组合，构成高阶组合特征。如果组合特征维度较高，可以使用低维向量表示特征，达到降维的目的，类似于稠密词向量的思想。

如何有效地找到组合特征？使用原始输入特征构造决策树，每一条从根结点到叶节点的路径都可以看成是一种特征组合的方式。然后使用这些特征去编码原始数据。

## 文本特征

### 文本表示模型

- 词袋模型和 N-gram 模型

  词袋模型将文章看作是一袋子词，忽略词序。以文本库中总词数为长度，构造长向量，其中每一维代表词表中的一个词，该维的权重反应对应的词在文章中的重要程度。常用 TF-IDF 计算权重：

  $$TF-IDF(t,d)=TF(t,d) \times IDF(t)$$
  
  单个的词忽略了词之间的内在关系，丢失了词序信息，往往不能很好的表示文本。N-gram 则将连续出现的 n 个词也作为一个特征放到向量表示中去，保留了一些词序信息，缺点就是会导致向量维度增加。一般情况下，权衡向量表示效果和维度，n 不超过 3 。

- 主题模型

  主题模型用于从文本库中发现有代表性的主题，得到每个主题上面词的分布特性，并且能够计算出每篇文章的主题分布。详情见主题模型一章。

- 词嵌入

  核心思想是将词映射成低维稠密向量。每一维可以看作是一个隐含的主题，只不过不像主题模型那样直观。

  在深度学习中，每一个隐藏层可以看作是在自动进行特征工程，其输出对应着不同抽象层次的特征。CNN 和 RNN 之所以在文本表示中取得很好的效果，主要是由于抓住了文本的特征，更好地对文本进行建模，另一方面又减少了待学习的参数，加快了训练速度，并且降低了过拟合的风险。
